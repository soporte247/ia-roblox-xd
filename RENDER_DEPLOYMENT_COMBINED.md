# DataShark - Deploy en Render (Backend + Beta Model Juntos)

## ğŸ“‹ OpciÃ³n 1: Mismo Web Service (Recomendado)

**Ventajas:**
- âœ… Un solo Web Service = menos costo
- âœ… ComunicaciÃ³n interna (localhost:8000)
- âœ… ConfiguraciÃ³n centralizada
- âœ… FÃ¡cil de mantener

### Paso 1: Actualizar Render Dashboard

En https://dashboard.render.com:

1. **Actualizar servicio existente (datashark-ia2)**
   - Build Command: `pip install -r mini-lemonade/ai-beta/requirements.txt && npm install --prefix mini-lemonade/backend`
   - Start Command: `node start-combined.js`

2. **Environment Variables** (En el servicio):
   ```
   BETA_MODEL_BASE_URL=http://localhost:8000
   BETA_MODEL_API_KEY=
   BETA_MODEL_NAME=datashark-beta
   ```

### Paso 2: Push a GitHub

```bash
git add start-combined.js Procfile .env.example
git commit -m "Add combined start script for backend + beta model on Render"
git push
```

Render detectarÃ¡ el cambio y harÃ¡ deploy automÃ¡tico.

### Paso 3: Verificar Deploy

```
Logs en Render:
- âœ… Backend starting on port 3000
- âœ… Beta model starting on port 8000
- âœ… Application startup complete
```

Acceso:
- Backend: https://datashark-ia2.onrender.com
- Health check: https://datashark-ia2.onrender.com/api/health
- Metrics: https://datashark-ia2.onrender.com/api/metrics

---

## ğŸ“‹ OpciÃ³n 2: Servicios Separados (Si prefieres)

Si necesitas flexibilidad para escalar independientemente:

1. **Web Service #1 (Backend)**
   - Start: `npm --prefix mini-lemonade/backend start`

2. **Web Service #2 (Beta Model)**
   - Start: `cd mini-lemonade/ai-beta && python -m uvicorn server:app --port $PORT`
   - Env: `BETA_MODEL_PATH=/tmp/datashark-model.pt`

3. **Backend .env**
   ```
   BETA_MODEL_BASE_URL=https://datashark-beta.onrender.com
   ```

---

## ğŸ—ï¸ Arquitectura (OpciÃ³n 1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Render Web Service: datashark-ia2      â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Node.js Backend (Port 3000)      â”‚  â”‚
â”‚  â”‚ - API Endpoints                  â”‚  â”‚
â”‚  â”‚ - Database                       â”‚  â”‚
â”‚  â”‚ - Authentication                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    â†“                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Python Beta Model (Port 8000)    â”‚  â”‚
â”‚  â”‚ - LLM Inference                  â”‚  â”‚
â”‚  â”‚ - OpenAI-compatible API          â”‚  â”‚
â”‚  â”‚ - Chat Completions               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    PUBLIC: Port 443 (HTTPS)
```

---

## ğŸ“š Flujo de Datos

```
Plugin (Lua)
    â†“ HTTPS
https://datashark-ia2.onrender.com
    â†“
Backend Node.js (Port 3000)
    â†“ HTTP localhost
http://localhost:8000
    â†“
Beta Model Python (Port 8000)
    â†“
LLM Inference
    â†“
Response JSON
```

---

## âš™ï¸ ConfiguraciÃ³n Final

### Backend .env (Ya en Render)
```env
NODE_ENV=production
PORT=3000
DATABASE_URL=./database.sqlite
BETA_MODEL_BASE_URL=http://localhost:8000
BETA_MODEL_API_KEY=
BETA_MODEL_NAME=datashark-beta
```

### No necesita cambios adicionales
- Roblox OAuth ya configurado
- DeepSeek fallback configurado
- Ollama fallback configurado
- Base de datos SQLite automÃ¡tica

---

## ğŸ§ª Testing Post-Deploy

```bash
# Health check
curl https://datashark-ia2.onrender.com/api/health

# Metrics
curl https://datashark-ia2.onrender.com/api/metrics

# Generate with beta model
curl -X POST https://datashark-ia2.onrender.com/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "crea un sistema de ataque",
    "type": "attack"
  }'
```

---

## ğŸ“Š Monitoreo

### Logs en Render Dashboard
- Servicios iniciando
- Errores de conexiÃ³n
- Uso de memoria

### MÃ©tricas (GET /api/metrics)
```json
{
  "ai": {
    "beta": {
      "calls": 42,
      "successes": 40,
      "failures": 2,
      "successRate": 95.24,
      "avgTime": 2.5
    }
  }
}
```

---

## ğŸ†˜ Troubleshooting

**Error: "Port 8000 in use"**
- Render ya tiene el puerto reservado, no hay conflicto

**Error: "Beta model not responding"**
- Verificar logs de Python en Render
- Revisar BETA_MODEL_PATH existe

**Error: "Connection timeout"**
- Aumentar timeout en generator.js (default: 30s)
- Verificar ambos procesos estÃ¡n corriendo

---

## âœ… Checklist Final

- [ ] Descargar start-combined.js y Procfile
- [ ] Actualizar Build Command en Render
- [ ] Actualizar Start Command en Render
- [ ] AÃ±adir environment variables
- [ ] Commit y push a GitHub
- [ ] Verificar deploy automÃ¡tico
- [ ] Test health endpoints
- [ ] Monitorear logs por 5 minutos
- [ ] Plugin Roblox conectando correctamente

---

**Status:** ğŸŸ¢ Listo para Deploy
**Costo:** Mismo que antes (1 servicio, no 2)
**Tiempo Setup:** <5 minutos

Siguiente: Commit y push para deploy automÃ¡tico en Render âœ¨
